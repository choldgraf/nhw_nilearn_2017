{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pylab as plt\n",
    "import os.path as op\n",
    "path_data = op.join(op.expanduser('~'), 'nilearn_data/')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation in neuroimaging\n",
    "\n",
    "This notebook is a slight extension of the Haxby decoding tutorial...it focuses on a few cross-validation methods you might employ in your model fit, and covers some best-practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load / display the data we'll work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "# By default the 2nd subject will be fetched\n",
    "haxby_dataset = datasets.fetch_haxby()\n",
    "fmri_filename = haxby_dataset.func[0]\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First subject functional nifti images (4D) are at: %s' %\n",
    "      fmri_filename)  # 4D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "plotting.plot_roi(mask_filename, bg_img=haxby_dataset.anat[0],\n",
    "                  cmap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll mask and vectorize the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "# Load the mask from disk\n",
    "masker = NiftiMasker(mask_img=mask_filename, standardize=True)\n",
    "\n",
    "# Fitting the transformer initializes it to operate on new data\n",
    "masker.fit(fmri_filename)\n",
    "\n",
    "# Now we'll transform our fMRI data\n",
    "fmri_masked = masker.transform(fmri_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"fmri_masked\" is a numpy array. It is 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its shape corresponds to the number of time-points x the number of\n",
    "voxels in the mask. Note that this is much fewer than the total number of voxels in the nifty image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the behavioral labels\n",
    "\n",
    "Now we'll load the behavioral labels for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load target information as string and give a numerical identifier to each\n",
    "labels = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like labels has the same length as our fMRI data, meaning that they share the same time-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll retrieve the behavioral targets from the labels. These will be the \"classes\" that we attempt to predict.\n",
    "\n",
    "Note that these labels aren't integers like before. That's fine - `sklearn` will try to be clever and convert these into integer representations when we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels['labels'].values[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict the analysis to cats and faces\n",
    "\n",
    "We'll take a subset of samples so that we're only including cats and faces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a mask w/ Pandas\n",
    "condition_mask = labels.eval('labels in [\"face\", \"cat\"]').values\n",
    "\n",
    "# Create a mask w/ Numpy\n",
    "# condition_mask = np.logical_or(target == b'face', target == b'cat')\n",
    "\n",
    "# We apply this mask in the sample direction to restrict the\n",
    "# classification to the face vs cat discrimination\n",
    "fmri_masked = fmri_masked[condition_mask]\n",
    "targets = labels[condition_mask]['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmri_masked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit our model\n",
    "\n",
    "Finally, we'll fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "print(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is already in the shape for `sklearn`, it's quite easy to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc.fit(fmri_masked, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svc.predict(fmri_masked)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our model\n",
    "\n",
    "The proper way to measure error rates or prediction accuracy is via\n",
    "cross-validation: leaving out some data and testing on it. There are many ways to do this.\n",
    "\n",
    "### ...by manually leaving out data\n",
    "\n",
    "Let's leave out the 30 last data points during training, and test the\n",
    "prediction on these 30 last points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(fmri_masked[:-30], targets[:-30])\n",
    "\n",
    "prediction = svc.predict(fmri_masked[-30:])\n",
    "print((prediction == targets[-30:]).sum() / float(len(targets[-30:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this seems unfortunate. We've now got 50% less data in order to fit the model. Ideally, we'd like to do two things:\n",
    "\n",
    "* Validate our model properly (aka, on held-out data not used in model fitting)\n",
    "* Use as much of our data as possible.\n",
    "\n",
    "It is difficult to satisfy both of these conditions properly, but *cross-validation* is one way of getting closer to this goal. \n",
    "\n",
    "### Cross-validation: Implementing a KFold loop\n",
    "\n",
    "We can split the data in train and test set repetitively in a `KFold`\n",
    "strategy. We'll create many cross-validation objects with `sklearn`. When one iterates through these, it returns different indices for training / test sets upon teach iteration.\n",
    "\n",
    "Let's visualize what this will look like below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KFold object\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_splits = 15\n",
    "cv = KFold(n_splits=n_splits)\n",
    "\n",
    "cv_sample = np.zeros([len(fmri_masked), n_splits])\n",
    "for ii, (tr,tt) in enumerate(cv.split(fmri_masked)):\n",
    "    cv_sample[tt, ii] = 1\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cv_sample, cmap='coolwarm', aspect='auto', interpolation='nearest')\n",
    "_ = ax.set(xlabel='Iteration', ylabel='Data Index',\n",
    "           title='Cross Validation indices\\n(red = Test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on each iteration we hold out a different subset of samples. Next, we'll loop through this object, fit a model on one subset of data, and then test it on the other subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in cv.split(fmri_masked):\n",
    "    svc.fit(fmri_masked[train], targets[train])\n",
    "    prediction = svc.predict(fmri_masked[test])\n",
    "    print(accuracy_score(targets[test], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all we want to do is score this model, note that `sklearn` has tools to perform cross-validation more succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_score = cross_val_score(svc, fmri_masked, targets)\n",
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we can speed things up to use all the CPUs of our computer\n",
    "with the n_jobs parameter. However, be careful in doing this on a cluster environment as you may be asking for resources not available to you.\n",
    "\n",
    "By default, cross_val_score uses a 3-fold KFold. We can control this by\n",
    "passing the \"cv\" object, here a 5-fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(svc, fmri_masked, targets,\n",
    "                           cv=cv.split(fmri_masked))\n",
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to visualize these as a histogram, to get an idea for the distribution of cross-validated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier_scores(scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(scores)\n",
    "    ax.axvline(.5, ls='--', c='r')\n",
    "    ax.set_xlabel('Model Score')\n",
    "plot_classifier_scores(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on cross-validating with time\n",
    "As neuroscientists, our data is often collected across time. This might be across a very short time-scale (milliseconds) or a long one (hours, or days). Either way, it is crucial to consider the relationships between datapoints as a function of time. Consider the following facts:\n",
    "\n",
    "* All time series data is correlated with itself (autocorrelated) to some degree\n",
    "* Confounding variables may be the same on one day of acquisition, and different on another day\n",
    "* The brain may have a different baseline internal state at the beginning of an experiment compared to the end.\n",
    "\n",
    "As we've mentioned before, you must **always test the model on \"new\" data**. This means that the training and test sets should share as *little information as possible*. In other words, anything in the training set that could give you information about the test set, but that is not related to the features of interest, will **bias** the model towards the wrong answer, or inflate your model score.\n",
    "\n",
    "\n",
    "## Leaving out recording sessions\n",
    "The best way to do cross-validation is to respect the structure of\n",
    "the experiment, for instance by leaving out full sessions of\n",
    "acquisition.\n",
    "\n",
    "The number of the session is stored in the CSV file with our\n",
    "behavioral data. We'll apply our condition mask, and then leave out one session at a time. To do this, we'll use a LeaveOneLabelOut object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out list of session number\n",
    "session_label = labels['chunks'][condition_mask]\n",
    "\n",
    "# Iterate through sessions, validating on a held-out session\n",
    "from sklearn.cross_validation import LeaveOneLabelOut\n",
    "cv = LeaveOneLabelOut(session_label)\n",
    "cv_score = cross_val_score(svc, fmri_masked, targets, cv=cv)\n",
    "plot_classifier_scores(cv_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
